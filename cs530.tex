\documentclass[article]{IEEEtran}
\usepackage[utf8]{inputenc}
\usepackage{outlines}
\usepackage{filecontents}
\usepackage{amsmath}
\usepackage[]{algorithm2e}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{graphicx}
\graphicspath{{./images/}}


\begin{document}
\title{Simulating and Surveying Fault Avoidance Efficacy of Majority Voting Systems with N-Version Software Systems}
\author{Allison Chilton\\\texttt{ allison.chilton@colostate.edu}}
\date{April 2021}

\maketitle


\begin{abstract}
This survey compares majority voting schemes for alike and disalike voter subsystems in an N-Version System. It explores how disalike voter schemes are likely to correct particular kinds of faults, perhaps more successfully than alike subsystems. It simulates the performance of disalike voting schemes and compares the results to a simulated alike voting scheme. It also explores how this architecture may make detecting certain kinds of faults more easy at the cost of making others more difficult.
\hfill\\\\
\textbf{Keywords: N-Version, Fault-Tolerant Software, Survey, Simulation}
\end{abstract}


\section{Introduction}
\par
Often majority voter schemes (such as TMR) are proposed to address faults within a system where the system design is assumed to be correct but environmental and mechanical faults are inevitable. Because of this, very often the voting mechanisms use identical subsystems to perform as voting agents. Further, in many formulations, it is considered a requirement that such agents be identical for the correctness of the algorithm. This does not protect against system design flaws and human error when defining requirements. This will explore using intentionally disalike subsystems with discrete outputs and voting cycles, also known as N-Version fault-tolerant software systems, to measure performance of its fault avoidance success. 
%Additionally, I would like to explore how following this paradigm might be incompatible with other approaches, such as the State Machine Replication of Byzantine fault avoidance. 
\par
The topic of N-Version fault-tolerant software schemes are well known in the literature. In brief, the fundamental idea is that a system that is designed independently by different teams, where each subsystem votes to form a consensus for a discretized time step, will show more fault-tolerance to design defects because teams will likely average out their misunderstandings of ambiguous or imperfect design requirements. There is disagreement about whether this is indeed effective, perhaps because people are likely to misinterpret something in the same wrong way. The author will review the state of the literature, contrast the arguments, and submit their own software simulation for various scenarios. 


\section{Literature Review}
% \subsection{Prework}
% Although the foundational works set the stage for the first forays into academic research for this field, even those papers must build on well known bodies of literature on the behavior of networks and general cryptographic principles. Exploring all of these papers recursively would result in a survey hundreds of pages long. For the sake of brevity, these papers are all outside the scope of this survey. However, I will list a handful of referenced papers and background that the reader may wish to familiarize themselves to more adequately understand the results of this survey: 
% \begin{outline}
%     \1 
%     \1 
%     \1 
% \end{outline}

\subsection{Foundational Works}

\subsubsection{Paper}
\hfill\\
\par
\textbf{Summary:} 
\par
\textbf{Discussion:}
\subsubsection{Paper}
\hfill\\
\par
\textbf{Summary:} 
\par
\textbf{Discussion:}
\subsection{Current Literature}


\section{Literature Matrix}
\section{Approach}
To gain some insight to the performance of these schemes, I wrote some random monte carlo simulations that have voter subsystems that artificially model being designed and built using similar but not identical interpretations of requirements. They have a randomly determined threshold to detect a particular domain of artificially inserted faults. There is also a parameter to test false positives. The idea is that in doing so, you are likely to average out the discrepancies in requirement interpretation and lead to more optimal performance. Additionally, I wrote simulations that use identical detections of particular fault domains, as a basis of comparison.  Additionally, I explored the effects of assigning weights to voter agents, in situations where a particular unit could be rigorously verified and tested, augmented by a higher quantity of lower quality backup devices. Finally, we'll analyze how this approach might be incompatible with other approaches, and the pro/cons of when it might be appropriate to use one approach or the other given your requirements. 

\begin{algorithm}
    \KwData{\newline
        \textbf{fset:} a set of faults and whether they are active
        \textbf{random\_subsystem:} a subsystem voter with a random probability of missing a fault or falsely detecting a fault when not present
        \textbf{random\_system:} a system with identical or non-identical subsystem voters
    }
    \KwResult{a collection of random trial results}
    results := \For{iter..trials }{
        votes := \ForEach{random\_subsystem in random\_system}{
            \ForEach{fault in fset}{
                \eIf{fault present}{hit\_prob := $P(\overline{FaultMissed} \cap FalsePositive)$ }
                {hit\_prob := $P(FalsePositive)$ }

                \eIf{uniform\_chance $<$ hit\_prob}{
                    collect vote := detected
                }
                {
                    collect vote := not-detected
                }
            }
        }
        collect result := majority\_vote(votes)

    }
    \Return{results}
    \caption{N-Modular Monte Carlo Simulation Approach}
\end{algorithm}

\section{Results}
In order to avoid sample bias, I needed to randomly generate several different situations. However, this causes the random distribution to cancel out its discrepant outliers, likely making differentiating the merit of either approach more difficult when testing the average performance. A better metric is to measure the variance and/or standard deviation of all performances in the distribution to determine the probability of being off nominal given a randomly “manufactured” device.
\par
There are static results in a table that describe a nominal setup, as well as a variant set of setups that vary a particular field to observe how the different types of configurations adjust their various measurements - correct percentage, standard deviation, p-values - based on the variation of that particular field. The variances change the parameter to be -+50\% the nominal setup.


\input{fig1.tex}
\input{plots.tex}

\textit{Insert more plots varying other things}

\subsection{Discussion}
\par You can see a trend in both the tables and plots. Our initial hypothesis was correct - in most cases the unweighted disalike (aka the N-Modular) system performed better than the alike. How you interpret performance here is subjective - if you look at the percent correct they are well within the margin of error and with more trials its likely the averaged out result would be even less pronounced. However, the variance / standard deviation of the population is the particular trend that we should observe. The standard deviation is almost always consistently lower by a not statistically insignifcant amount (as shown by the p-values in the table and plots). This confirms our initial conjecture: given any random system configuration, you are statistically more likely to be closer to the mean if you have an N-Modular system.

\section{Other works}
\section{Conclusion and Future Work}
\section{References}

\begin{filecontents}[overwrite]{cs530bib.bib}


\end{filecontents}
\bibliographystyle{unsrt}
%\bibliography{cs530bib}
\appendix
\section{Source Code}
Full source for simulation available at \url{ https://github.com/allisonChilton/n_modular_voting_sim/blob/master/tp_sim.py}

\end{document}


